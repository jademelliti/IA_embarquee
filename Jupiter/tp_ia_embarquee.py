# -*- coding: utf-8 -*-
"""TP_IA_EMBARQUEE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19KPXsvec7-4X4XXqsaFGvNUl6qZ98dgZ

## **PRACTICAL SESSION 1** — Deep Learning for predictive maintenance

The dataset used is the **AI4I 2020** Predictive Maintenance Dataset, which contains 10,000 instances of industrial sensor data. Each instance represents the operating condition of a machine and is associated with a label indicating whether a failure has occurred and, if so, what type of failure it is.

The 5 possible labels are:



*   **TWF**: Tool Wear Failure
*   **HDF**: Heat Dissipation Failure
*   **PWF**: Power Failure
*   **OSF**: Overstrain Failure
*   **RNF**: Random Failure


The data is available on eCAMPUS as CSV file called: "ai4i2020.csv"

Here is the start of the ai4i2020.csv file : 
UDI,Product ID,Type,Air temperature [K],Process temperature [K],Rotational speed [rpm],Torque [Nm],Tool wear [min],Machine failure,TWF,HDF,PWF,OSF,RNF
1,M14860,M,298.1,308.6,1551,42.8,0,0,0,0,0,0,0
2,L47181,L,298.2,308.7,1408,46.3,3,0,0,0,0,0,0
3,L47182,L,298.1,308.5,1498,49.4,5,0,0,0,0,0,0
4,L47183,L,298.2,308.6,1433,39.5,7,0,0,0,0,0,0
5,L47184,L,298.2,308.7,1408,40.0,9,0,0,0,0,0,0
6,M14865,M,298.1,308.6,1425,41.9,11,0,0,0,0,0,0

## **PRACTICAL SESSION Goal** — Ceate a deep leanring model allowing to realize a predictive maintenance mission

"""
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.datasets import make_classification
from imblearn.under_sampling import RandomUnderSampler

"""**QUESTION:** Load dataset and display some lines of the csv file."""

df = pd.read_csv("ai4i2020.csv")
print(df.head())
# Remove cases where RNF == 1 pour machine failure =0 ou =1
df = df[df['RNF'] == 0]

# Remove cases where Machine failure == 1 but no specific failure type is recorded
failure_types_ok = ['TWF', 'HDF', 'PWF', 'OSF']
df = df[~((df['Machine failure'] == 1) & (df[failure_types_ok].sum(axis=1) == 0))]

# Drop RNF column
df = df.drop(columns=['RNF'])
#add NF column = l'inverse de machine failure
df['NF'] = (df['Machine failure'] == 0).astype(int)
# Drop RNF column
df = df.drop(columns=['Machine failure'])

# Display first lines of the dataset
print(df.head())
print(len(df))  # Nombre total de lignes

X = df[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]


Y = df[['TWF', 'HDF', 'PWF', 'OSF', 'NF']]


df = df[df[failure_types_ok].sum(axis=1) < 2]

conditions = [
    Y['TWF'] == 1,
    Y['HDF'] == 1,
    Y['PWF'] == 1,
    Y['OSF'] == 1,
    Y['NF'] == 1
]

choices = [0, 1, 2, 3, 4]

Y_single = np.select(conditions, choices, default=0)

# Convertir les étiquettes rééchantillonnées en format multilabel (one-hot encoding)
#Y_res_multilabel = np.zeros((Y_single.size, Y.shape[1]))
#Y_res_multilabel[np.arange(Y_single.size), Y_single] = 1

#Attention, avec cette version si une sample est 0 1 1 0 0 elle sera mise dans la classe 1 et la classe 2

# Appliquer le sur-échantillonnage avec SMOTE
smote = SMOTE(random_state=42)
X_res, Y_res = smote.fit_resample(X, Y_single)


#NORMALISATION DES DONNEES !!!!!!!!
scaler = StandardScaler()
X_res_normalized = scaler.fit_transform(X_res)



# Convertir les étiquettes rééchantillonnées en format multilabel (one-hot encoding)
Y_res_multilabel = np.zeros((Y_res.size, Y.shape[1]))
Y_res_multilabel[np.arange(Y_res.size), Y_res] = 1


# Diviser les données
X_train, X_val, Y_train, Y_val = train_test_split(X_res_normalized, Y_res_multilabel, test_size=0.30, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=42)

np.save('Xtest.npy', X_test)
np.save('Ytest.npy', Y_test)



inputs = tf.keras.Input(shape=(5,)) #my input layer
x = tf.keras.layers.Flatten()(inputs) #cf. question below...
x = tf.keras.layers.Dense(20, activation='tanh')(x) #a first hidden layer with 20 neurons
outputs = tf.keras.layers.Dense(5, activation='softmax')(x) # my output layer
#Then, I define my model with the input layer, the output layer and a name
my_model = tf.keras.Model(inputs=inputs, outputs=outputs, name="my_model")

#PRINT A SUMMARY OF THE ARCHITECTURE OF MY MODEL WITH THE NUMBER OF TRAINABLE PARAMETERS
my_model.summary()

'''
(1) Set the "optimizer" [pick 'adam', 'sgd' or 'rmsprop']
(2) Set the loss [cf. lesson #3, we pick the categorical cross-entropy]
(3) Set the final performance metric to evaluate the model
'''
print("Shape of X:", X.shape)
print("Shape of Y:", Y.shape)

my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


print(X_train.shape)
print(X_test.shape)



# (1) DEFINE THE ARCHITECTURE OF MY MODEL
#first, I define all the layers and the way they are connected
inputs = tf.keras.Input(shape=(5,)) #my input layer
x = tf.keras.layers.Flatten()(inputs) #cf. question below...
x = tf.keras.layers.Dense(64, activation='relu')(x) #a first hidden layer with 64 neurons
x = tf.keras.layers.Dense(64, activation='relu')(x) #a second hidden layer with 64 neurons
outputs = tf.keras.layers.Dense(5, activation='softmax')(x) # my output layer
#Then, I define my model with the input layer, the output layer and a name
my_mlp_model = tf.keras.Model(inputs=inputs, outputs=outputs, name="my_mlp_model")

#PRINT A SUMMARY OF THE ARCHITECTURE OF MY MODEL WITH THE NUMBER OF TRAINABLE PARAMETERS
my_mlp_model.summary()

# (2) DEFINE THE TRAINING HYPER-PARAMETERS WITH THE "COMPILE" METHOD:
'''
(1) Set the "optimizer" [pick 'adam', 'sgd' or 'rmsprop']
(2) Set the loss [cf. lesson #3, we pick the categorical cross-entropy]
(3) Set the final performance metric to evaluate the model
'''

my_mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# (3) NOW, LET'S TRAIN ON MY DATA WITH THE "FIT" METHOD
'''
(1) Set the number of epochs
(2) Set the size of the (mini)batch
(3) Set the training dataset ==> here, X_train with Y_train
(4) Set the validation dataset (X_val, Y_val)
'''

nb_epochs=50
batch_size=100
training_history = my_mlp_model.fit(X_train,Y_train,
                                    validation_data=(X_val, Y_val),
                                    epochs=nb_epochs,
                                    batch_size=batch_size)


#COMPUTE THE ACCURACY ON THE TRAINING AND TEST SETS
loss_train, acc_train = my_mlp_model.evaluate(X_train, Y_train, batch_size=batch_size)
loss_test, acc_test = my_mlp_model.evaluate(X_test, Y_test, batch_size=batch_size)

print("Performance on the TRAIN set, ACCURACY=",acc_train)
print("Performance on the TEST set, ACCURACY=",acc_test)

my_mlp_model.save('notre_model_aj.h5')

history = my_mlp_model.fit(
    X_train, Y_train,                # Données d'entraînement
    epochs=50,                        # Nombre d'époques
    batch_size=32,                    # Taille du lot
    validation_data=(X_val, Y_val)  # Données de validation
)

